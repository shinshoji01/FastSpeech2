transformer:
  encoder_layer: 4
  encoder_head: 2
  encoder_hidden: 256
  decoder_layer: 6
  decoder_head: 2
  decoder_hidden: 256
  conv_filter_size: 1024
  conv_kernel_size: [9, 1]
  encoder_dropout: 0.2
  decoder_dropout: 0.2
  encoder_architecture: "FastSpeech2" # "FastSpeech2" or "BERT"
  BERT_pretrained: "/work/Git/cuhksz-phd/notebooks/BERT_pretrained_models/BERT/Base/checkpoint-500000/"

variance_predictor:
  filter_size: 256
  kernel_size: 3
  dropout: 0.5

variance_embedding:
  pitch_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the pitch values are not normalized during preprocessing
  energy_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  ed_quantization: "linear" # support 'linear' or 'log', 'log' is allowed only if the energy values are not normalized during preprocessing
  ed_min: 0
  ed_max: 1.0
  n_bins: 256
  prosody_order: ["ed", "duration", "pitch", "energy"]

# gst:
#   use_gst: False
#   conv_filters: [32, 32, 64, 64, 128, 128]
#   gru_hidden: 128
#   token_size: 128
#   n_style_token: 10
#   attn_head: 4

multi_speaker: False

max_seq_len: 1000

vocoder:
  model: "HiFi-GAN" # support 'HiFi-GAN', 'MelGAN'
  speaker: "0001" # support  'LJSpeech', 'universal'
  hifigan_config: "/work/Git/FastSpeech2/hifigan/config.json" 
  hifigan_pretrained_model: "/work/Git/cuhksz-phd/notebooks/hifigan_parameters/Tacotron2/g_00110000"

ed:
  include_ed: False
  combination: "addition"
  phonemes_words_utterance: [True, True, True]
  concatenation_embedding_size: 64
gst:
  include_gst: False
plpm:
  include_plpm: False
  num_gaussians: 100
  mel_hidden: 256
